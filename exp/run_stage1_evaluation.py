# -*- coding: utf-8 -*-
"""
Script for Stage 1: Training and Preliminary Evaluation of AgriTabPFNRegressor.

This script performs the following steps:
1. Loads preprocessed training and testing data from .npz files.
   (Corrected to use 'features', 'targets', 'info' keys)
   (Made info loading more robust)
   (Added check for 3D shape of X_train/X_test)
2. Initializes the AgriTabPFNRegressor model with baseline parameters.
   (Corrected preprocessor parameter names for AgriTabPFNRegressor)
3. Trains the model on the training data.
4. Evaluates the model on the test data using RMSE, MAE, and R-squared metrics.
5. Logs relevant information and results.
"""

import argparse
import logging
import os
import time
from typing import Dict, Tuple, Any, Optional, List

import numpy as np
import torch
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Attempt to import AgriTabPFNRegressor.
# Adjust the import path if your project structure is different.
try:
    from tabpfn.agri_tabpfn import AgriTabPFNRegressor
except ImportError:
    logging.error(
        "Failed to import AgriTabPFNRegressor. "
        "Ensure 'tabpfn' is installed or 'PYTHONPATH' is set correctly "
        "to include the 'src' directory of your TabPFN project."
    )
    try:
        from src.tabpfn.agri_tabpfn import AgriTabPFNRegressor
        logging.info("Imported AgriTabPFNRegressor from src.tabpfn.agri_tabpfn")
    except ImportError as e_inner:
        raise ImportError(
            "Could not import AgriTabPFNRegressor from common paths. "
            "Please check your environment and PYTHONPATH. Original error: " + str(e_inner)
        )


# --- Configuration ---
RANDOM_SEED = 42  # For reproducibility

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(module)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)


def set_random_seeds(seed: int) -> None:
    """
    Sets random seeds for NumPy and PyTorch for reproducibility.

    Args:
        seed: The random seed to use.
    """
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    logger.info(f"Set random seeds to {seed}")


def load_data(
    train_file_path: str, test_file_path: str
) -> Tuple[
    np.ndarray, np.ndarray, Optional[Dict[str, Any]],
    np.ndarray, np.ndarray, Optional[Dict[str, Any]],
]:
    """
    Loads training and testing data from .npz files.

    The .npz files are expected to be generated by preprocess_agri_datasets.py,
    containing keys: 'features', 'targets', and 'info'.
    'features' (X_train, X_test) are expected to be 3D arrays.

    Args:
        train_file_path: Path to the training data .npz file.
        test_file_path: Path to the testing data .npz file.

    Returns:
        A tuple containing (X_train, y_train, train_info, X_test, y_test, test_info).
        *_info can be None if not found in the .npz file or if loading fails.

    Raises:
        FileNotFoundError: If .npz files are not found.
        KeyError: If expected 'features' or 'targets' keys are missing.
        ValueError: If 'features' arrays are not 3-dimensional.
    """
    logger.info(f"Loading training data from: {train_file_path}")
    try:
        train_data_npz = np.load(train_file_path, allow_pickle=True)
    except FileNotFoundError:
        logger.error(f"Training data file not found: {train_file_path}")
        raise
    except Exception as e:
        logger.error(f"Error loading training data from {train_file_path}: {e}")
        raise

    logger.info(f"Loading testing data from: {test_file_path}")
    try:
        test_data_npz = np.load(test_file_path, allow_pickle=True)
    except FileNotFoundError:
        logger.error(f"Test data file not found: {test_file_path}")
        raise
    except Exception as e:
        logger.error(f"Error loading testing data from {test_file_path}: {e}")
        raise

    expected_features_key = 'features'
    expected_targets_key = 'targets'
    expected_info_key = 'info'

    # --- Load Training Data ---
    actual_train_keys = set(train_data_npz.files)
    logger.info(f"Keys found in training NPZ file ({train_file_path}): {list(actual_train_keys)}")

    if expected_features_key not in actual_train_keys:
        raise KeyError(
            f"Expected feature key '{expected_features_key}' not found in training NPZ file: {train_file_path}. "
            f"Available keys: {list(actual_train_keys)}"
        )
    X_train = train_data_npz[expected_features_key]

    if X_train.ndim != 3:
        raise ValueError(
            f"Training features 'X_train' from {train_file_path} are not 3-dimensional. "
            f"Expected shape (n_samples, n_feature_rows, n_timesteps), got {X_train.shape}. "
            "Please check preprocess_agri_datasets.py to ensure it produces 3D feature arrays."
        )

    if expected_targets_key not in actual_train_keys:
        raise KeyError(
            f"Expected label key '{expected_targets_key}' not found in training NPZ file: {train_file_path}. "
            f"Available keys: {list(actual_train_keys)}"
        )
    y_train = train_data_npz[expected_targets_key]

    train_info = None
    if expected_info_key in actual_train_keys:
        loaded_item = train_data_npz[expected_info_key]
        if isinstance(loaded_item, np.ndarray) and loaded_item.size == 1:
            try:
                content = loaded_item.item()
                if isinstance(content, dict):
                    train_info = content
                else:
                    logger.warning(
                        f"Extracted item for '{expected_info_key}' from {train_file_path} "
                        f"is not a dictionary (type: {type(content)}). Value: {content}. Setting info to None."
                    )
            except ValueError as e:
                 logger.error(
                    f"ValueError when calling .item() on '{expected_info_key}' from {train_file_path} "
                    f"(size 1 array). Error: {e}. Setting info to None."
                )
        elif isinstance(loaded_item, dict): 
            train_info = loaded_item
            logger.info(f"'{expected_info_key}' in {train_file_path} was loaded directly as a dictionary.")
        else: 
            logger.error(
                f"Cannot extract a single dictionary for '{expected_info_key}' from {train_file_path}. "
                f"Object type: {type(loaded_item)}, "
                f"Shape: {getattr(loaded_item, 'shape', 'N/A')}, "
                f"Size: {getattr(loaded_item, 'size', 'N/A')}. "
                "This suggests the 'info' field in the NPZ is not a dictionary as expected. Setting info to None."
            )
    else:
        logger.warning(f"Optional key '{expected_info_key}' not found in {train_file_path}. Proceeding without train_info.")

    # --- Load Testing Data ---
    actual_test_keys = set(test_data_npz.files)
    logger.info(f"Keys found in testing NPZ file ({test_file_path}): {list(actual_test_keys)}")

    if expected_features_key not in actual_test_keys:
        raise KeyError(
            f"Expected feature key '{expected_features_key}' not found in testing NPZ file: {test_file_path}. "
            f"Available keys: {list(actual_test_keys)}"
        )
    X_test = test_data_npz[expected_features_key]

    if X_test.ndim != 3:
        raise ValueError(
            f"Testing features 'X_test' from {test_file_path} are not 3-dimensional. "
            f"Expected shape (n_samples, n_feature_rows, n_timesteps), got {X_test.shape}. "
            "Please check preprocess_agri_datasets.py to ensure it produces 3D feature arrays."
        )

    if expected_targets_key not in actual_test_keys:
        raise KeyError(
            f"Expected label key '{expected_targets_key}' not found in testing NPZ file: {test_file_path}. "
            f"Available keys: {list(actual_test_keys)}"
        )
    y_test = test_data_npz[expected_targets_key]

    test_info = None
    if expected_info_key in actual_test_keys:
        loaded_item = test_data_npz[expected_info_key]
        if isinstance(loaded_item, np.ndarray) and loaded_item.size == 1:
            try:
                content = loaded_item.item()
                if isinstance(content, dict):
                    test_info = content
                else:
                    logger.warning(
                        f"Extracted item for '{expected_info_key}' from {test_file_path} "
                        f"is not a dictionary (type: {type(content)}). Value: {content}. Setting info to None."
                    )
            except ValueError as e:
                 logger.error(
                    f"ValueError when calling .item() on '{expected_info_key}' from {test_file_path} "
                    f"(size 1 array). Error: {e}. Setting info to None."
                )
        elif isinstance(loaded_item, dict):
            test_info = loaded_item
            logger.info(f"'{expected_info_key}' in {test_file_path} was loaded directly as a dictionary.")
        else:
            logger.error(
                f"Cannot extract a single dictionary for '{expected_info_key}' from {test_file_path}. "
                f"Object type: {type(loaded_item)}, "
                f"Shape: {getattr(loaded_item, 'shape', 'N/A')}, "
                f"Size: {getattr(loaded_item, 'size', 'N/A')}. "
                "This suggests the 'info' field in the NPZ is not a dictionary as expected. Setting info to None."
            )
    else:
        logger.warning(f"Optional key '{expected_info_key}' not found in {test_file_path}. Proceeding without test_info.")

    logger.info(f"Training data shapes: X_train={X_train.shape}, y_train={y_train.shape}")
    if train_info:
        logger.info(f"Training info keys: {list(train_info.keys())}")
    logger.info(f"Testing data shapes: X_test={X_test.shape}, y_test={y_test.shape}")
    if test_info:
        logger.info(f"Test info keys: {list(test_info.keys())}")
        
    train_data_npz.close()
    test_data_npz.close()

    return X_train, y_train, train_info, X_test, y_test, test_info


def initialize_model(
    model_params: Dict[str, Any],
    train_info: Optional[Dict[str, Any]], 
    X_train_shape: Tuple, 
    cli_static_row_index: Optional[int],
    cli_max_temporal_length: Optional[int],
) -> AgriTabPFNRegressor:
    """
    Initializes the AgriTabPFNRegressor model.
    """
    static_row_index = cli_static_row_index
    if static_row_index is None and train_info is not None:
        static_row_index = train_info.get('static_feature_row_index',
                                          train_info.get('static_row_index_actual',
                                          train_info.get('static_row_index')))
        if static_row_index is not None:
            logger.info(f"Using static_row_index from train_info: {static_row_index}")

    if static_row_index is None: 
        static_row_index = -1 
        logger.warning(
            f"static_row_index not found in train_info or CLI args, defaulting to {static_row_index}."
        )

    max_temporal_length = cli_max_temporal_length
    if max_temporal_length is None and train_info is not None:
        max_temporal_length = train_info.get('max_len', 
                                             train_info.get('max_temporal_len_actual',
                                             train_info.get('max_temporal_length')))
        if max_temporal_length is not None:
            logger.info(f"Using max_temporal_length from train_info: {max_temporal_length}")
    
    if max_temporal_length is None: 
        if len(X_train_shape) == 3: # X_train_shape is (n_samples, n_feature_rows, n_timesteps)
            max_temporal_length = X_train_shape[2] # n_timesteps (number of columns for temporal data)
            logger.info(
                f"max_temporal_length not specified via CLI or found in train_info. "
                f"Inferred from X_train.shape[2]: {max_temporal_length}"
            )
        else:
            # This case should ideally be caught by the X_train.ndim != 3 check in load_data
            logger.error(
                "X_train is not 3D, so max_temporal_length cannot be inferred from its shape. "
                "This indicates a problem with data loading or preprocessing."
            )
            # AgriTabPFNRegressor's preprocessor will likely fail if max_temporal_length is None and data isn't as expected.
            # We proceed with None, but a failure is anticipated in the preprocessor.
            max_temporal_length = None


    final_model_params = model_params.copy()
    final_model_params['static_row_index'] = static_row_index
    final_model_params['max_temporal_length'] = max_temporal_length

    logger.info(f"Initializing AgriTabPFNRegressor with parameters: {final_model_params}")
    model = AgriTabPFNRegressor(**final_model_params)
    return model


def train_model_on_data(
    model: AgriTabPFNRegressor, X_train: np.ndarray, y_train: np.ndarray
) -> AgriTabPFNRegressor:
    """
    Trains the AgriTabPFNRegressor model.
    """
    logger.info("Starting model training...")
    start_time = time.time()
    model.fit(X_train, y_train)
    end_time = time.time()
    training_duration = end_time - start_time
    logger.info(f"Model training completed in {training_duration:.2f} seconds.")
    return model


def evaluate_model(
    model: AgriTabPFNRegressor, X_test: np.ndarray, y_test: np.ndarray
) -> Dict[str, float]:
    """
    Evaluates the trained model on the test set.
    """
    logger.info("Starting model evaluation...")
    start_time = time.time()
    y_pred = model.predict(X_test)
    end_time = time.time()
    prediction_duration = end_time - start_time
    logger.info(f"Model prediction completed in {prediction_duration:.2f} seconds.")

    if y_pred.ndim > 1 and y_pred.shape[1] == 1:
        y_pred = y_pred.squeeze(axis=1)
    if y_test.ndim > 1 and y_test.shape[1] == 1:
        y_test = y_test.squeeze(axis=1)

    if y_pred.shape != y_test.shape:
        logger.error(
            f"Shape mismatch between y_pred ({y_pred.shape}) and y_test ({y_test.shape}). "
            "Cannot calculate metrics."
        )
        return {
            "rmse": float('nan'), "mae": float('nan'), "r2": float('nan'),
            "prediction_duration_sec": prediction_duration,
        }
    
    if len(y_test) == 0:
        logger.warning("Test set is empty. Cannot compute metrics.")
        return {
            "rmse": float('nan'), "mae": float('nan'), "r2": float('nan'),
            "prediction_duration_sec": prediction_duration,
        }

    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    metrics = {"rmse": rmse, "mae": mae, "r2": r2, "prediction_duration_sec": prediction_duration}
    logger.info(f"Evaluation metrics: {metrics}")
    return metrics


def main(args: argparse.Namespace) -> None:
    """
    Main function to orchestrate the training and evaluation process.
    """
    set_random_seeds(RANDOM_SEED)

    if args.device == "cuda" and not torch.cuda.is_available():
        logger.warning("CUDA specified but not available. Falling back to CPU.")
        device = "cpu"
    else:
        device = args.device
    logger.info(f"Using device: {device}")

    X_train, y_train, train_info, X_test, y_test, test_info = load_data(
        args.train_file_path, args.test_file_path
    )

    # Corrected parameter names for AgriTabPFNRegressor's preprocessor arguments
    base_model_params = {
        "temporal_hidden_dim": 128,
        "static_hidden_dims": [64],
        "fusion_strategy": "gated",
        "encoded_embedding_dim": 128, 
        "standardize_static_features": True,   # Corrected
        "standardize_temporal_features": True, # Corrected
        "handle_missing_static": "mean",       # Corrected
        "handle_missing_temporal": "mean",     # Corrected
        "n_estimators": args.n_estimators,
        "device": device,
        "fit_mode": "fit_preprocessors", 
        "random_state": RANDOM_SEED,
    }

    model = initialize_model(
        base_model_params,
        train_info, 
        X_train.shape,
        args.static_row_index,
        args.max_temporal_length
    )

    trained_model = train_model_on_data(model, X_train, y_train)
    metrics = evaluate_model(trained_model, X_test, y_test)

    logger.info("--- Stage 1: Preliminary Evaluation Summary ---")
    for metric_name, value in metrics.items():
        logger.info(f"{metric_name.upper()}: {value:.4f}")
    logger.info("-----------------------------------------------")

    if args.output_dir:
        if not os.path.exists(args.output_dir):
            os.makedirs(args.output_dir)
            logger.info(f"Created output directory: {args.output_dir}")
        
        metrics_file = os.path.join(args.output_dir, "stage1_metrics.txt")
        with open(metrics_file, "w") as f:
            f.write(f"Experiment Parameters:\n")
            f.write(f"  Train file: {args.train_file_path}\n")
            f.write(f"  Test file: {args.test_file_path}\n")
            f.write(f"  Static row index (CLI): {args.static_row_index}\n")
            f.write(f"  Max temporal length (CLI): {args.max_temporal_length}\n")
            f.write(f"  Device: {device}\n")
            f.write(f"  N estimators: {args.n_estimators}\n")
            f.write(f"  Model effective static_row_index: {getattr(trained_model, 'static_row_index', 'N/A')}\n")
            f.write(f"  Model effective max_temporal_length: {getattr(trained_model, 'max_temporal_length', 'N/A')}\n")
            f.write("\nEvaluation Metrics:\n")
            for metric_name, value in metrics.items():
                f.write(f"  {metric_name.upper()}: {value:.4f}\n")
        logger.info(f"Metrics and parameters saved to {metrics_file}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Stage 1: Train and evaluate AgriTabPFNRegressor."
    )
    parser.add_argument(
        "--train_file_path", type=str, required=True,
        help="Path to the training data .npz file (e.g., train_processed.npz). "
             "Expected keys: 'features', 'targets', 'info'. 'features' must be 3D.")
    parser.add_argument(
        "--test_file_path", type=str, required=True,
        help="Path to the testing data .npz file (e.g., test_processed.npz). "
             "Expected keys: 'features', 'targets', 'info'. 'features' must be 3D.")
    parser.add_argument(
        "--output_dir", type=str, default=None,
        help="Directory to save logs and output artifacts (optional).")
    parser.add_argument(
        "--static_row_index", type=int, default=None, 
        help="Row index for static features. Overrides 'info' from NPZ. Defaults to -1 if not found.")
    parser.add_argument(
        "--max_temporal_length", type=int, default=None, 
        help="Max temporal sequence length. Overrides 'info' from NPZ. If None, inferred from data.")
    parser.add_argument(
        "--n_estimators", type=int, default=8,
        help="Number of ensemble configurations for TabPFN.")
    parser.add_argument(
        "--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu",
        choices=["cuda", "cpu"], help="Device for training ('cuda' or 'cpu').")

    parsed_args = parser.parse_args()
    
    try:
        main(parsed_args)
    except ImportError as e:
        logger.error(f"Import error: {e}")
        logger.error("Ensure TabPFN and agri components are installed/accessible via PYTHONPATH.")
    except FileNotFoundError as e:
        logger.error(f"File not found error: {e}")
    except KeyError as e: # For missing 'features' or 'targets'
        logger.error(f"Key error loading data from NPZ: {e}")
        logger.error("Ensure .npz files use 'features', 'targets' keys.")
    except ValueError as e: # For data shape issues or other value errors
        logger.error(f"Value error during script execution: {e}")
    except Exception as e: # Catch-all for other unexpected errors
        logger.exception(f"An unexpected error occurred: {e}")

